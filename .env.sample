# ============================================================================
# AI Doc Gen - Environment Configuration
# ============================================================================
# 
# This file contains all configurable environment variables for the AI Doc Gen
# application. Copy this file to `.env` and update the values as needed.
#
# üìã Quick Setup Checklist:
# 1. Copy this file: cp .env.sample .env
# 2. Set your LLM API keys and base URLs
# 3. Configure GitLab integration (for cronjob mode)
# 4. Adjust retry/timeout settings based on your environment
# 5. Enable observability tools if needed (Langfuse)
#
# üöÄ For rate-limited environments, increase retry values:
#   - ANALYZER_AGENT_RETRIES=5
#   - TOOL_FILE_READER_MAX_RETRIES=5
#   - HTTP_RETRY_MAX_ATTEMPTS=10
# ============================================================================

# ------------- Core Application Settings ----------
PYTHONPATH=src
ENVIRONMENT=development                  # Options: development, staging, production

# ------------- Observability & Telemetry ----------
# Langfuse provides LLM observability and analytics
ENABLE_LANGFUSE=false                   # Set to 'true' to enable Langfuse tracking
OTEL_SDK_DISABLED=false                 # OpenTelemetry SDK control

# Langfuse Configuration (only needed if ENABLE_LANGFUSE=true)
LANGFUSE_PUBLIC_KEY=YOUR_PUBLIC_KEY_HERE
LANGFUSE_SECRET_KEY=YOUR_SECRET_KEY_HERE
LANGFUSE_HOST=https://YOUR_LANGFUSE_HOST_HERE
OTEL_EXPORTER_OTLP_ENDPOINT=YOUR_OTEL_ENDPOINT_HERE

# ============================================================================
# ü§ñ LLM AGENTS CONFIGURATION (Powered by LiteLLM)
# ============================================================================
# 
# Each agent can use a DIFFERENT LLM provider with its own API key and base URL.
# Model names follow LiteLLM format for automatic provider detection:
#   - OpenAI: "gpt-4o", "gpt-4-turbo", "o1-preview"
#   - Azure OpenAI: "azure/<deployment-name>"
#   - Anthropic: "anthropic/claude-3-opus", "claude-sonnet-4-20250514"
#   - AWS Bedrock: "bedrock/anthropic.claude-3-sonnet"
#   - Google Vertex: "vertex_ai/gemini-pro"
#   - Ollama: "ollama/llama2"
#   - Together AI: "together_ai/mistralai/Mixtral-8x7B"
#
# See full provider list: https://docs.litellm.ai/docs/providers
# ============================================================================

# ------------- Provider API Keys (Set keys for providers you use) ----------
# LiteLLM automatically uses these based on the model prefix.
# You can use provider-native env vars OR per-agent overrides (see below).

# OpenAI (for models like "gpt-4o", "o1-preview")
OPENAI_API_KEY=

# Anthropic (for models like "claude-sonnet-4-20250514", "anthropic/claude-3-opus")
ANTHROPIC_API_KEY=

# Azure OpenAI (for models like "azure/my-gpt4-deployment")
AZURE_API_KEY=
AZURE_API_BASE=                         # e.g., https://my-resource.openai.azure.com
AZURE_API_VERSION=2024-10-21            # Azure API version

# AWS Bedrock (for models like "bedrock/anthropic.claude-3-sonnet")
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_REGION_NAME=

# ============================================================================
# AGENT CONFIGURATION
# ============================================================================
# Each agent REQUIRES a model name. API key/base URL are OPTIONAL overrides
# that take precedence over the provider-native env vars above.
#
# Example: Using 3 different providers
#   ANALYZER_LLM_MODEL=azure/gpt-4o-deployment    (uses AZURE_* env vars)
#   DOCUMENTER_LLM_MODEL=claude-sonnet-4-20250514 (uses ANTHROPIC_API_KEY)
#   AI_RULES_LLM_MODEL=gpt-4o                     (uses OPENAI_API_KEY)
# ============================================================================

# ------------- Analyzer Agent (Code Analysis) ----------
# Performs code structure, dependency, data flow, and API analysis
ANALYZER_LLM_MODEL=claude-sonnet-4-20250514          # REQUIRED: LiteLLM model format
# ANALYZER_LLM_API_KEY=                              # Optional: Override provider API key
# ANALYZER_LLM_BASE_URL=                             # Optional: Override provider base URL (e.g., https://api.anthropic.com)
# ANALYZER_LLM_API_VERSION=                          # Optional: For Azure OpenAI
ANALYZER_PARALLEL_TOOL_CALLS=true                    # Enable parallel tool execution

# Analyzer Agent Behavior Settings
ANALYZER_AGENT_RETRIES=2                # Retries per analysis agent on failure
ANALYZER_LLM_TIMEOUT=180                # Request timeout in seconds (3 minutes)
ANALYZER_LLM_MAX_TOKENS=8192            # Maximum tokens in LLM responses
ANALYZER_LLM_TEMPERATURE=0.0            # Response randomness (0.0=deterministic, 1.0=creative)
ANALYZER_MAX_WORKERS=0                  # Max concurrent workers (0=auto-detect CPU count)

# ------------- Documenter Agent (README Generation) ----------
# Generates comprehensive README.md files
DOCUMENTER_LLM_MODEL=claude-sonnet-4-20250514        # REQUIRED: LiteLLM model format
# DOCUMENTER_LLM_API_KEY=                            # Optional: Override provider API key
# DOCUMENTER_LLM_BASE_URL=                           # Optional: Override provider base URL (e.g., https://api.anthropic.com)
# DOCUMENTER_LLM_API_VERSION=                        # Optional: For Azure OpenAI
DOCUMENTER_PARALLEL_TOOL_CALLS=true                  # Enable parallel tool execution

# Documenter Agent Behavior Settings
DOCUMENTER_AGENT_RETRIES=2              # Retries for documenter agent on failure
DOCUMENTER_LLM_TIMEOUT=180              # Request timeout in seconds
DOCUMENTER_LLM_MAX_TOKENS=8192          # Maximum tokens in LLM responses
DOCUMENTER_LLM_TEMPERATURE=0.0          # Response randomness (0.0=deterministic, 1.0=creative)

# ------------- AI Rules Generator (CLAUDE.md, AGENTS.md, .cursor/rules/) ----------
# Creates configuration files for AI coding assistants
# If not specified, defaults to DOCUMENTER settings
# AI_RULES_LLM_MODEL=gpt-4o                          # Optional: Defaults to DOCUMENTER_LLM_MODEL
# AI_RULES_LLM_API_KEY=                              # Optional: Override provider API key
# AI_RULES_LLM_BASE_URL=                             # Optional: Override provider base URL (e.g., https://api.openai.com/v1)
# AI_RULES_LLM_API_VERSION=                          # Optional: For Azure OpenAI
AI_RULES_PARALLEL_TOOL_CALLS=true                    # Enable parallel tool execution

# AI Rules Generator Behavior Settings
AI_RULES_AGENT_RETRIES=2                # Retries for AI rules generator on failure
AI_RULES_LLM_TIMEOUT=240                # Request timeout in seconds (4 minutes)
AI_RULES_LLM_MAX_TOKENS_MARKDOWN=8192   # Max tokens for CLAUDE.md/AGENTS.md generation
AI_RULES_LLM_MAX_TOKENS_CURSOR=16384    # Max tokens for Cursor rules generation
AI_RULES_LLM_TEMPERATURE=0.0            # Response randomness (0.0=deterministic, 1.0=creative)

# ============================================================================
# üîß RETRY & RESILIENCE CONFIGURATION
# ============================================================================

# ------------- Agent Tools Settings ----------
# These settings control the retry behavior for internal tools used by agents
TOOL_FILE_READER_MAX_RETRIES=2          # File reading tool retry attempts
TOOL_LIST_FILES_MAX_RETRIES=2           # File listing tool retry attempts  

# ------------- HTTP Retry Client ----------
# Controls retry behavior for all HTTP requests to LLM providers
# üí° Increase these values if you encounter rate limiting issues
HTTP_RETRY_MAX_ATTEMPTS=5               # Total HTTP retry attempts before failure
HTTP_RETRY_MULTIPLIER=1                 # Exponential backoff multiplier (1s‚Üí2s‚Üí4s‚Üí8s)
HTTP_RETRY_MAX_WAIT_PER_ATTEMPT=60      # Maximum wait between attempts (seconds)
HTTP_RETRY_MAX_TOTAL_WAIT=300           # Maximum total wait time (5 minutes)

# ‚ö° Rate Limiting Solutions:
# If you encounter "max retries exceeded" or "request limit" errors:
# - Increase HTTP_RETRY_MAX_ATTEMPTS to 10+
# - Increase ANALYZER_AGENT_RETRIES to 5+
# - Increase timeout values (ANALYZER_LLM_TIMEOUT=300+)

# ============================================================================
# üîó SCM PROVIDER CONFIGURATION (Required for Cronjob Mode)
# ============================================================================

# Provider Selection - determines which SCM system to use
# Options: gitlab, bitbucket_server
SCM_PROVIDER=gitlab

# ------------- Provider-Agnostic Settings ----------
# These settings work across all SCM providers
SCM_API_URL=https://git.divar.cloud           # Your SCM instance URL
SCM_GIT_USER_NAME=AI Analyzer                 # Display name for commits/PRs
SCM_GIT_USER_USERNAME=agent_doc               # Username for filtering PRs
SCM_GIT_USER_EMAIL=YOUR_EMAIL_HERE            # Email for Git commits

# ------------- GitLab Configuration (when SCM_PROVIDER=gitlab) ----------
GITLAB_OAUTH_TOKEN=YOUR_GITLAB_TOKEN_HERE     # OAuth token (preferred)
GITLAB_PRIVATE_TOKEN=                         # Private token (alternative to OAuth)

# üîë GitLab Token Permissions Required:
# - api (full API access)
# - read_repository (read repo contents)
# - write_repository (create branches, commits)

# ------------- Bitbucket Server Configuration (when SCM_PROVIDER=bitbucket_server) ----------
# Note: This is for Bitbucket SERVER (self-hosted), NOT Bitbucket Cloud
BITBUCKET_TOKEN=YOUR_BITBUCKET_TOKEN_HERE     # Personal access token (preferred)
BITBUCKET_USERNAME=                           # Username (for basic auth, alternative to token)
BITBUCKET_PASSWORD=                           # Password (for basic auth, alternative to token)

# üîë Bitbucket Token Permissions Required:
# - Repository read (read repo contents)
# - Repository write (create branches, commits)
# - Pull request write (create PRs)

# ============================================================================
# üìä LOGGING & DEBUGGING
# ============================================================================

# Logging Configuration
CONSOLE_LOG_LEVEL=WARNING               # Console output level
FILE_LOG_LEVEL=INFO                     # File logging level

# Available log levels (from most to least verbose):
# DEBUG    - Detailed debugging information
# INFO     - General information messages
# WARNING  - Warning messages (default for console)
# ERROR    - Error messages only
# CRITICAL - Critical errors only

# üêõ For debugging issues:
# Set CONSOLE_LOG_LEVEL=DEBUG to see detailed operation logs

# ============================================================================
# üöÄ DEPLOYMENT-SPECIFIC RECOMMENDATIONS
# ============================================================================

# Development Environment:
# - Use DEBUG log levels for troubleshooting
# - Lower retry counts for faster feedback
# - Enable Langfuse for observability

# Production Environment:
# - Use WARNING/ERROR log levels
# - Higher retry counts for resilience
# - Monitor with Langfuse/OTEL
# - Set appropriate timeout values based on your LLM provider

# Rate-Limited Environments:
# - ANALYZER_AGENT_RETRIES=5+
# - HTTP_RETRY_MAX_ATTEMPTS=10+
# - HTTP_RETRY_MAX_TOTAL_WAIT=600+
# - ANALYZER_LLM_TIMEOUT=300+

# High-Performance Environments:
# - ANALYZER_PARALLEL_TOOL_CALLS=true
# - DOCUMENTER_PARALLEL_TOOL_CALLS=true
# - Higher ANALYZER_LLM_MAX_TOKENS values
# - Lower timeout values for faster failures
